{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b>  Importing Libraries </b></div>\n\nIn this section, we import all the libraries required for data processing, feature extraction, and model building. These libraries help us perform tasks like loading and manipulating data, tokenizing text, building neural networks.\n","metadata":{}},{"cell_type":"code","source":"# Core libraries\nimport numpy as np\nimport pandas as pd\n\n# NLP preprocessing\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nnltk.download('omw-1.4')\nnltk.download('stopwords')\n\n# Scikit-learn for splitting data, evaluation and models \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\n\n# TensorFlow/Keras for deep learning (BiLSTM)\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    LSTM, Bidirectional, Dense, Dropout, Masking\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:29:39.992036Z","iopub.execute_input":"2025-06-30T20:29:39.992438Z","iopub.status.idle":"2025-06-30T20:29:49.685865Z","shell.execute_reply.started":"2025-06-30T20:29:39.992395Z","shell.execute_reply":"2025-06-30T20:29:49.684965Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 3. Loading the Dataset </b></div>\n\nIn this step, we load the IMDB dataset, which contains 50,000 movie reviews labeled as either positive or negative.  \nEach review is stored as a text entry along with its sentiment label.\n\nWe use `pandas.read_csv()` to load the dataset into a DataFrame for further processing.\n","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\ndf = pd.read_csv(path)\n\n# Show the first 5 rows\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:29:49.687249Z","iopub.execute_input":"2025-06-30T20:29:49.687795Z","iopub.status.idle":"2025-06-30T20:29:51.015178Z","shell.execute_reply.started":"2025-06-30T20:29:49.687766Z","shell.execute_reply":"2025-06-30T20:29:51.014209Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 4. Data Preprocessing </b></div>\n\n","metadata":{}},{"cell_type":"code","source":"# Define English stopwords\nstop_words = set(stopwords.words('english'))\n\n# Define preprocessing function\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n    # Remove non-alphabetic characters\n    text = re.sub(r'[^a-z\\s]', '', text)\n    # Remove extra spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    # Remove stopwords\n    tokens = text.split()\n    tokens = [word for word in tokens if word not in stop_words]\n    return \" \".join(tokens)\n\n# Apply preprocessing to the 'review' column\ndf['clean_review'] = df['review'].apply(preprocess_text)\n\n# Show sample after preprocessing\ndf[['review', 'clean_review']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:29:54.954247Z","iopub.execute_input":"2025-06-30T20:29:54.954612Z","iopub.status.idle":"2025-06-30T20:30:01.65443Z","shell.execute_reply.started":"2025-06-30T20:29:54.95458Z","shell.execute_reply":"2025-06-30T20:30:01.653415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Keep only Positive and Negative samples\ndf['sentiment'] = df['sentiment'].map({'negative': 0, 'positive': 1})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:30:01.655707Z","iopub.execute_input":"2025-06-30T20:30:01.655965Z","iopub.status.idle":"2025-06-30T20:30:01.664689Z","shell.execute_reply.started":"2025-06-30T20:30:01.655938Z","shell.execute_reply":"2025-06-30T20:30:01.663717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Split into train (80%) and test (20%)\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['sentiment'])\n\n# Step 2: Split train into train (80% of 80%) and val (20% of 80%) â†’ 64% train, 16% val\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df['sentiment'])\n\n# Show sizes\nprint(f\"Train size: {len(train_df)}\")\nprint(f\"Validation size: {len(val_df)}\")\nprint(f\"Test size: {len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:30:06.000532Z","iopub.execute_input":"2025-06-30T20:30:06.001167Z","iopub.status.idle":"2025-06-30T20:30:06.055612Z","shell.execute_reply.started":"2025-06-30T20:30:06.001129Z","shell.execute_reply":"2025-06-30T20:30:06.054641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 5. Tokenization</b></div>\n","metadata":{}},{"cell_type":"code","source":"# Apply word_tokenize to the cleaned review column\ntrain_df['tokens'] = train_df['clean_review'].apply(word_tokenize)\nval_df['tokens'] = val_df['clean_review'].apply(word_tokenize)\ntest_df['tokens'] = test_df['clean_review'].apply(word_tokenize)\n\n# Show sample tokens\ntrain_df[['clean_review', 'tokens']].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:30:10.594123Z","iopub.execute_input":"2025-06-30T20:30:10.594502Z","iopub.status.idle":"2025-06-30T20:30:35.417299Z","shell.execute_reply.started":"2025-06-30T20:30:10.594471Z","shell.execute_reply":"2025-06-30T20:30:35.416334Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 6. Text Representation Methods </b></div>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Join the tokens back into full sentences (as TF-IDF expects raw text input)\ntrain_texts = train_df['tokens'].apply(lambda x: ' '.join(x))\nval_texts = val_df['tokens'].apply(lambda x: ' '.join(x))\ntest_texts = test_df['tokens'].apply(lambda x: ' '.join(x))\n\n# Initialize TF-IDF vectorizer with a maximum of 5000 features\ntfidf = TfidfVectorizer(max_features=5000)\n\n# Fit the vectorizer on training data and transform it\nX_train_tfidf = tfidf.fit_transform(train_texts)\n\n# Transform validation and test sets using the fitted vectorizer\nX_val_tfidf = tfidf.transform(val_texts)\nX_test_tfidf = tfidf.transform(test_texts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:30:35.419109Z","iopub.execute_input":"2025-06-30T20:30:35.419844Z","iopub.status.idle":"2025-06-30T20:30:40.092679Z","shell.execute_reply.started":"2025-06-30T20:30:35.419798Z","shell.execute_reply":"2025-06-30T20:30:40.091971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\n# Train a Word2Vec model on the tokenized text\nw2v_model = Word2Vec(sentences=train_df['tokens'], vector_size=200, window=6, min_count=2)\n\n# Function to convert tokens to sequence of word vectors\ndef tokens_to_sequence(tokens):\n    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n    return vectors\n\n# Apply tokens_to_sequence to convert tokens to sequences of word vectors\nX_train_seq_w2v = train_df['tokens'].apply(tokens_to_sequence).tolist()\nX_val_seq_w2v = val_df['tokens'].apply(tokens_to_sequence).tolist()\nX_test_seq_w2v = test_df['tokens'].apply(tokens_to_sequence).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:30:40.093898Z","iopub.execute_input":"2025-06-30T20:30:40.094188Z","iopub.status.idle":"2025-06-30T20:31:25.875891Z","shell.execute_reply.started":"2025-06-30T20:30:40.094161Z","shell.execute_reply":"2025-06-30T20:31:25.875066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 7. Model Training </b></div>\n\nA model is a mathematical structure that learns patterns from data.  \nTraining a model means teaching it to make predictions by learning from labeled examples (inputs and outputs).\n","metadata":{}},{"cell_type":"code","source":"# Initialize Logistic Regression model\nlog_reg = LogisticRegression(max_iter=250, C=0.1, penalty='l2')\n# Train the model\nlog_reg.fit(X_train_tfidf, train_df['sentiment'])\n\n# Predict on training set\nlog_train_preds = log_reg.predict(X_train_tfidf)\ntrain_accuracy = accuracy_score(train_df['sentiment'], log_train_preds)\n\n# Predict on validation set\nlog_val_preds = log_reg.predict(X_val_tfidf)\nval_accuracy = accuracy_score(val_df['sentiment'], log_val_preds)\n\n# Print results\nprint(\"Logistic Regression Accuracy:\")\nprint(f\"Train Accuracy: {train_accuracy:.4f}\")\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:32:19.354606Z","iopub.execute_input":"2025-06-30T20:32:19.355126Z","iopub.status.idle":"2025-06-30T20:32:19.376953Z","shell.execute_reply.started":"2025-06-30T20:32:19.355074Z","shell.execute_reply":"2025-06-30T20:32:19.375859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define max sequence length and vector size (same as Word2Vec vector size)\nmax_len = 280\nvector_size = 200\n\n# Pad sequences to make them all of the same shape (max_len x vector_size)\nX_train_seq_padded = pad_sequences(X_train_seq_w2v, maxlen=max_len,\n                                   dtype='float32',padding='post',\n                                   truncating='post', value=0.0)\nX_val_seq_padded = pad_sequences(X_val_seq_w2v, maxlen=max_len,\n                                 dtype='float32',padding='post',\n                                 truncating='post', value=0.0)\nX_test_seq_padded = pad_sequences(X_test_seq_w2v, maxlen=max_len,\n                                  dtype='float32',padding='post',\n                                  truncating='post', value=0.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:32:23.764078Z","iopub.execute_input":"2025-06-30T20:32:23.765091Z","iopub.status.idle":"2025-06-30T20:32:28.991589Z","shell.execute_reply.started":"2025-06-30T20:32:23.76505Z","shell.execute_reply":"2025-06-30T20:32:28.990544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build the BiLSTM model\nmodel = Sequential([\n    Masking(mask_value=0.0, input_shape=(max_len, vector_size)), \n    Bidirectional(LSTM(256, return_sequences=True, \n                       kernel_regularizer=regularizers.l2(0.0005))),\n    Dropout(0.4),\n    Bidirectional(LSTM(128, kernel_regularizer=regularizers.l2(0.0005))),\n    Dropout(0.4),\n    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n    Dropout(0.4),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n    Dropout(0.4),\n    Dense(1, activation='sigmoid') \n])\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy',\n              optimizer= Adam(learning_rate=0.001),\n              metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train_seq_padded,\n                    train_df['sentiment'],\n                    epochs=12,\n                    batch_size=64,\n                    validation_data=(X_val_seq_padded, val_df['sentiment']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:32:30.281077Z","iopub.execute_input":"2025-06-30T20:32:30.281416Z","iopub.status.idle":"2025-06-30T20:42:08.790059Z","shell.execute_reply.started":"2025-06-30T20:32:30.281388Z","shell.execute_reply":"2025-06-30T20:42:08.789205Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 8. Export Predictions for Evaluation </b></div>\n","metadata":{}},{"cell_type":"code","source":"# Logistic Regression Predictions\nlogistic_probs = log_reg.predict_proba(X_test_tfidf)\nlogistic_preds = log_reg.predict(X_test_tfidf)\n\nlogistic_results_df = pd.DataFrame({\n    'review': test_df['review'].values,\n    'true_label': test_df['sentiment'].values,\n    'predicted_label': logistic_preds,\n    'prob_negative': logistic_probs[:, 0],\n    'prob_positive': logistic_probs[:, 1]\n})\n\nlogistic_results_df.to_csv(\"logistic_preds.csv\", index=False)\nprint(\"Saved logistic_preds.csv\")\n\n\n# BiLSTM Predictions\nbilstm_probs = model.predict(X_test_seq_padded)\nbilstm_preds = (bilstm_probs > 0.5).astype(int).flatten()\n\nbilstm_results_df = pd.DataFrame({\n    'review': test_df['review'].values,\n    'true_label': test_df['sentiment'].values,\n    'predicted_label': bilstm_preds,\n    'prob_negative': 1 - bilstm_probs.flatten(),\n    'prob_positive': bilstm_probs.flatten()\n})\n\nbilstm_results_df.to_csv(\"bilstm_preds.csv\", index=False)\nprint(\"Saved bilstm_preds.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T20:44:47.770609Z","iopub.execute_input":"2025-06-30T20:44:47.771562Z","iopub.status.idle":"2025-06-30T20:45:01.839833Z","shell.execute_reply.started":"2025-06-30T20:44:47.771524Z","shell.execute_reply":"2025-06-30T20:45:01.838981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# <div style=\"text-align:center; background: #03045e; padding: 7px; border-radius:10px 10px; font-size: 1.5em; color: #e3f2fd; cursor: pointer;font-family: cursive;\"><b> 9. Conclusion </b></div>\n\n","metadata":{}}]}